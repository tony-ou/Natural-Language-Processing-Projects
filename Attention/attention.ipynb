{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import numpy as np \n",
    "from random import shuffle\n",
    "from time import time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Word Averaging Binary ClassiÔ¨Åer (20 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Implementation and Experimentation (15 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data,max_sequence_len):\n",
    "    corpus = data.split(\"\\n\")\n",
    "    predictors = []\n",
    "    labels = []\n",
    "    mean_len = []\n",
    "    max_sequence_len\n",
    "    \n",
    "    for line in corpus:\n",
    "        sent, score = line.split(\"\\t\")\n",
    "        labels.append(int(score))\n",
    "        token_list = tokenizer.texts_to_sequences([sent])[0]\n",
    "        mean_len.append(max_sequence_len/len(token_list))\n",
    "        token_list = np.array(pad_sequences([token_list], maxlen=max_sequence_len, padding='post'))[0]\n",
    "        predictors.append(torch.tensor(token_list,dtype=torch.int64))\n",
    "    return predictors, labels, mean_len\n",
    "\n",
    "def evaluate(model,predictors,labels,mean_len,mb_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            total_pred = 0\n",
    "            total_correct = 0\n",
    "            total_loss =0 \n",
    "            sequences = [i for i in range(len(predictors))]\n",
    "            last_size = len(sequences) % mb_size\n",
    "            for mb in range(1+ len(sequences)//mb_size):\n",
    "                if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).\\\n",
    "                                view(mb_size,1).repeat(1,d).cuda()\n",
    "                else: \n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.float).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.float).\\\n",
    "                                view(last_size,1).repeat(1,d).cuda()\n",
    "                    mb_size = last_size \n",
    "                \n",
    "                probs = model(batch.cuda(),mean_l)\n",
    "                loss =  - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.000001).view(mb_size)))\n",
    "                total_loss += loss.item()\n",
    "                total_pred += mb_size \n",
    "                predict = list(map(lambda x : x >= 0.5,probs.view(mb_size).cpu().data.numpy() ))\n",
    "                target = target.cpu().data.numpy() \n",
    "                for j in range(len(predict)):\n",
    "                    if predict[j] == target[j]:\n",
    "                        total_correct += 1     \n",
    "    model.train()    \n",
    "    return total_pred,total_correct,total_loss/total_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary and data preprocessing\n",
    "tokenizer = Tokenizer(lower=False,filters='\\t')\n",
    "train = open('senti.train.tsv').read()\n",
    "dev = open('senti.dev.tsv').read()\n",
    "test = open('senti.test.tsv').read()\n",
    "data = train + dev + test\n",
    "corpus = data.split(\"\\n\")\n",
    "max_sequence_len = 0 \n",
    "corpus = list(map(lambda x: x[:-1],corpus))\n",
    "for i in corpus: \n",
    "    max_sequence_len = max(max_sequence_len, len(re.split(' |\\t',i))-1)\n",
    "    \n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index)\n",
    "\n",
    "predictors, labels,mean_len = dataset_preparation(train,max_sequence_len)\n",
    "d_predictors, d_labels, d_mean_len = dataset_preparation(dev,max_sequence_len)\n",
    "t_predictors, t_labels, t_mean_len = dataset_preparation(test,max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "total prediction: 872, train_accuracy: 0.8061926605504587, train avg_loss: 0.42769300992335746\n",
      "total prediction: 872, validation_accuracy: 0.6938073394495413, validation avg_loss: 0.5665919075318433\n",
      "epoch: 1\n",
      "total prediction: 872, train_accuracy: 0.6938073394495413, train avg_loss: 0.5665919075318433\n",
      "total prediction: 872, validation_accuracy: 0.7958715596330275, validation avg_loss: 0.4552579795548675\n",
      "epoch: 2\n",
      "total prediction: 872, train_accuracy: 0.7958715596330275, train avg_loss: 0.4552579795548675\n",
      "total prediction: 872, validation_accuracy: 0.8107798165137615, validation avg_loss: 0.4274528190630292\n",
      "epoch: 3\n",
      "total prediction: 872, train_accuracy: 0.8107798165137615, train avg_loss: 0.4274528190630292\n",
      "total prediction: 872, validation_accuracy: 0.8142201834862385, validation avg_loss: 0.4275120947885951\n",
      "epoch: 4\n",
      "total prediction: 872, train_accuracy: 0.8142201834862385, train avg_loss: 0.4275120947885951\n",
      "total prediction: 872, validation_accuracy: 0.8176605504587156, validation avg_loss: 0.43367425618915384\n",
      "epoch: 5\n",
      "total prediction: 872, train_accuracy: 0.8176605504587156, train avg_loss: 0.43367425618915384\n",
      "total prediction: 872, validation_accuracy: 0.8130733944954128, validation avg_loss: 0.4463384908820511\n",
      "epoch: 6\n",
      "total prediction: 872, train_accuracy: 0.8130733944954128, train avg_loss: 0.4463384908820511\n",
      "total prediction: 872, validation_accuracy: 0.8153669724770642, validation avg_loss: 0.46897957385133165\n",
      "epoch: 7\n",
      "total prediction: 872, train_accuracy: 0.8153669724770642, train avg_loss: 0.46897957385133165\n",
      "total prediction: 872, validation_accuracy: 0.8153669724770642, validation avg_loss: 0.486369001209189\n",
      "epoch: 8\n",
      "total prediction: 872, train_accuracy: 0.8153669724770642, train avg_loss: 0.486369001209189\n",
      "total prediction: 872, validation_accuracy: 0.8153669724770642, validation avg_loss: 0.5036775570943814\n",
      "epoch: 9\n",
      "total prediction: 872, train_accuracy: 0.8153669724770642, train avg_loss: 0.5036775570943814\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.5328666226032677\n",
      "61.95553994178772\n"
     ]
    }
   ],
   "source": [
    "# model implementation and training\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "class WordAvg(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordAvg, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim,padding_idx=0)\n",
    "        self.embeddings.weight.data.uniform_(-1, 1)\n",
    "        self.embeddings.weight.data[0] =  torch.zeros_like(self.embeddings.weight.data[0])\n",
    "        self.linear1 = nn.Linear(embedding_dim, 1,bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs,mean_l):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = torch.mean(embeds,dim=1)/mb_size\n",
    "        out = torch.mul(out, mean_l)\n",
    "        out = self.linear1(out)\n",
    "        probs = self.sigmoid(out)\n",
    "        return probs\n",
    "\n",
    "d =100\n",
    "model = WordAvg(total_words, d)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.cuda()\n",
    " \n",
    "a = time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    mb_size = 32\n",
    "    sequences = [i for i in range(len(predictors))]\n",
    "    shuffle(sequences)\n",
    "    last_size = len(sequences) % mb_size\n",
    "    for mb in range(1+len(sequences)//mb_size):\n",
    "        if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).\\\n",
    "                                view(mb_size,1).repeat(1,d).cuda()\n",
    "        else: \n",
    "            if last_size == 0:\n",
    "                break\n",
    "            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "            mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.double).\\\n",
    "                        view(last_size,1).repeat(1,d).cuda()\n",
    "            mb_size = last_size\n",
    "        \n",
    "        model.zero_grad()\n",
    "        probs = model(batch.cuda(),mean_l)\n",
    "        \n",
    "        loss = - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.000001).view(mb_size)))\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.05)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model.state_dict(), \"./model_earlystop_\" + str(epoch))    \n",
    "    total_pred,total_correct,avg_loss= evaluate(model,predictors, labels,mean_len,mb_size)\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    print(\"total prediction: {}, train_accuracy: {}, train avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "    total_pred,total_correct,avg_loss= evaluate(model,d_predictors, d_labels,d_mean_len,mb_size)\n",
    "    print(\"total prediction: {}, validation_accuracy: {}, validation avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_accuracy: 0.8153669724770642, dev avg_loss: 0.486369001209189\n",
      "test_accuracy: 0.8039538714991763, test avg_loss: 0.44997723478330875\n"
     ]
    }
   ],
   "source": [
    "# test set evaluation\n",
    "model = WordAvg(total_words, d)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(\"./model_earlystop_7\"))\n",
    "total_pred,total_correct,avg_loss= evaluate(model,t_predictors, t_labels,t_mean_len,mb_size)\n",
    "print(\"dev_accuracy: {}, dev avg_loss: {}\".format(0.8153669724770642,0.486369001209189))\n",
    "print(\"test_accuracy: {}, test avg_loss: {}\".format(total_correct/total_pred,avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Analysis (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------largest 15 words with norms-----\n",
      "worst: 26.194583892822266\n",
      "unfunny: 25.550601959228516\n",
      "poorly: 25.409120559692383\n",
      "wonderful: 25.06730842590332\n",
      "suffers: 25.066425323486328\n",
      "devoid: 24.72293472290039\n",
      "touching: 24.331138610839844\n",
      "remarkable: 23.52764892578125\n",
      "mess: 23.515443801879883\n",
      "playful: 23.075441360473633\n",
      "heartwarming: 23.02166175842285\n",
      "terrific: 23.005342483520508\n",
      "unnecessary: 22.89522361755371\n",
      "flat: 22.829540252685547\n",
      "badly: 22.794445037841797\n",
      "\n",
      "------smallest 15 words with norms------\n",
      "Yimou: 4.877057075500488\n",
      "combination: 4.896606922149658\n",
      "Crimen: 4.912820816040039\n",
      "unplundered: 4.942306041717529\n",
      "Randall: 4.950387477874756\n",
      "Mixes: 4.953327178955078\n",
      "theater: 4.958885669708252\n",
      "arguable: 4.9900031089782715\n",
      "caring: 4.990123271942139\n",
      "medicine: 4.997435569763184\n",
      "lie: 5.021319389343262\n",
      "spouse: 5.029397964477539\n",
      "brassy: 5.034116268157959\n",
      "Birot: 5.051749229431152\n",
      "oatmeal: 5.053561687469482\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for param in model.parameters():\n",
    "    weights.append(param.data)\n",
    "embed_weights = weights[0]\n",
    "l2_norms = torch.norm(embed_weights,dim=1).cpu().data.numpy()\n",
    "sorted_norm = sorted(list(zip(range(len(l2_norms)),l2_norms)),key=lambda x: x[1])\n",
    "large =  sorted_norm[-15:]\n",
    "small =  sorted_norm[0:16] # account for 0\n",
    "\n",
    "print(\"------largest 15 words with norms-----\")\n",
    "for i in range(15):\n",
    "    print(\"{}: {}\".format(tokenizer.index_word[large[14-i][0]],large[14-i][1]))\n",
    "print(\"\")\n",
    "print(\"------smallest 15 words with norms------\")\n",
    "for i in range(16):\n",
    "    if small[i][0] != 0:\n",
    "        print(\"{}: {}\".format(tokenizer.index_word[small[i][0]],small[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"./model1\")\n",
    "ab = torch.load( \"./model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attention-Weighted Word Averaging (20 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Implementation and Experimentation (10 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "total prediction: 67349, train_accuracy: 0.8854474453963682, train avg_loss: 0.3273802132777774\n",
      "total prediction: 872, validation_accuracy: 0.7958715596330275, validation avg_loss: 0.4668108049882661\n",
      "epoch: 1\n",
      "total prediction: 67349, train_accuracy: 0.9246313976451024, train avg_loss: 0.2125090480989779\n",
      "total prediction: 872, validation_accuracy: 0.8256880733944955, validation avg_loss: 0.41458561278264455\n",
      "epoch: 2\n",
      "total prediction: 67349, train_accuracy: 0.9403109177567596, train avg_loss: 0.16834892617262087\n",
      "total prediction: 872, validation_accuracy: 0.8222477064220184, validation avg_loss: 0.4094048216255433\n",
      "epoch: 3\n",
      "total prediction: 67349, train_accuracy: 0.9481358297821794, train avg_loss: 0.14370701670992708\n",
      "total prediction: 872, validation_accuracy: 0.8211009174311926, validation avg_loss: 0.4197514059893582\n",
      "epoch: 4\n",
      "total prediction: 67349, train_accuracy: 0.9536147530030141, train avg_loss: 0.12745158337327117\n",
      "total prediction: 872, validation_accuracy: 0.8107798165137615, validation avg_loss: 0.43452998968439366\n",
      "epoch: 5\n",
      "total prediction: 67349, train_accuracy: 0.9576385692437898, train avg_loss: 0.11617352477354888\n",
      "total prediction: 872, validation_accuracy: 0.8107798165137615, validation avg_loss: 0.45829075957657\n",
      "epoch: 6\n",
      "total prediction: 67349, train_accuracy: 0.9610981603290324, train avg_loss: 0.10781024258490471\n",
      "total prediction: 872, validation_accuracy: 0.8038990825688074, validation avg_loss: 0.48089004711273614\n",
      "epoch: 7\n",
      "total prediction: 67349, train_accuracy: 0.9629393160997194, train avg_loss: 0.1016574162663175\n",
      "total prediction: 872, validation_accuracy: 0.8038990825688074, validation avg_loss: 0.503990524679149\n",
      "epoch: 8\n",
      "total prediction: 67349, train_accuracy: 0.9645429033838662, train avg_loss: 0.09664600571620886\n",
      "total prediction: 872, validation_accuracy: 0.805045871559633, validation avg_loss: 0.5312033399529413\n",
      "epoch: 9\n",
      "total prediction: 67349, train_accuracy: 0.9659831623335164, train avg_loss: 0.09281498052775786\n",
      "total prediction: 872, validation_accuracy: 0.8004587155963303, validation avg_loss: 0.5542632385131416\n",
      "236.62009143829346\n"
     ]
    }
   ],
   "source": [
    "# model implementation and training\n",
    "def masked_softmax(A, dim=1, epsilon=1e-5):\n",
    "    A_exp = torch.exp(A)\n",
    "    A_exp = A_exp * (A != 0).type(torch.FloatTensor).cuda() # this step masks\n",
    "    A_softmax = A_exp / torch.sum(A_exp,dim,keepdim=True)\n",
    "    return A_softmax\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim,padding_idx=0)\n",
    "        self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.embeddings.weight.data[0] =  torch.zeros_like(self.embeddings.weight.data[0])\n",
    "        \n",
    "        self.u = torch.nn.Parameter(torch.randn(embedding_dim).view(1,1,embedding_dim).cuda())\n",
    "        self.linear1 = nn.Linear(embedding_dim, 1,bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, inputs,mb_size):\n",
    "        embeds = self.embeddings(inputs)                \n",
    "        att =torch.nn.functional.cosine_similarity(self.u,embeds,dim=2)\n",
    "        att = masked_softmax(att,dim=1)\n",
    "        att =  torch.unsqueeze(att, 2)\n",
    "        att = att.repeat(1,1,100)\n",
    "        out = torch.mul(embeds, att)\n",
    "        out = torch.sum(out,dim=1)\n",
    "        out = self.linear1(out)\n",
    "        probs = self.sigmoid(out)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def evaluate2(model,predictors,labels,mb_size,max_sequence_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            total_pred = 0\n",
    "            total_correct = 0\n",
    "            total_loss =0 \n",
    "            sequences = [i for i in range(len(predictors))]\n",
    "            last_size = len(sequences) % mb_size\n",
    "            for mb in range(1+ len(sequences)//mb_size):\n",
    "                if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).cuda()    \n",
    "                else: \n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.float).cuda()\n",
    "                    mb_size = last_size \n",
    "                probs = model(batch.cuda(),max_sequence_len)\n",
    "                loss =  - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.000001).view(mb_size)))\n",
    "                total_loss += loss.item()\n",
    "                total_pred += mb_size \n",
    "                predict = list(map(lambda x : x >= 0.5,probs.view(mb_size).cpu().data.numpy() ))\n",
    "                target = target.cpu().data.numpy() \n",
    "                for j in range(len(predict)):\n",
    "                    if predict[j] == target[j]:\n",
    "                        total_correct += 1     \n",
    "    model.train()    \n",
    "    return total_pred,total_correct,total_loss/total_pred\n",
    "    \n",
    "d =100\n",
    "mb_size = 100\n",
    "model2 = Attention(total_words, d)\n",
    "optimizer = optim.Adam(model2.parameters(), lr = 0.0003)\n",
    "model2 = model2.cuda()\n",
    " \n",
    "a = time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    mb_size = 32\n",
    "    sequences = [i for i in range(len(predictors))]\n",
    "    shuffle(sequences)\n",
    "    last_size = len(sequences) % mb_size\n",
    "    for mb in range(1+len(sequences)//mb_size):\n",
    "        if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.float).cuda()\n",
    "        else: \n",
    "            if last_size == 0:\n",
    "                break\n",
    "            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.float).cuda()\n",
    "            mb_size = last_size\n",
    "        model2.zero_grad()\n",
    "        probs = model2(batch.cuda(),max_sequence_len)\n",
    "        loss = - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.00001).view(mb_size)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model2.state_dict(), \"./model_earlystop_\" + str(epoch))    \n",
    "    total_pred,total_correct,avg_loss= evaluate2(model2,predictors, labels,mb_size,max_sequence_len)\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    print(\"total prediction: {}, train_accuracy: {}, train avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "    total_pred,total_correct,avg_loss= evaluate2(model2,d_predictors, d_labels,mb_size,max_sequence_len)\n",
    "    print(\"total prediction: {}, validation_accuracy: {}, validation avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_accuracy: 0.8256880733944955, dev avg_loss: 0.41458561278264455\n",
      "test_accuracy: 0.8121911037891268, test avg_loss: 0.4002359582460038\n"
     ]
    }
   ],
   "source": [
    "# test set evaluation\n",
    "model2 = Attention(total_words, d)\n",
    "model2.cuda()\n",
    "model2.load_state_dict(torch.load(\"./model_earlystop_2\"))\n",
    "total_pred,total_correct,avg_loss= evaluate2(model2,t_predictors, t_labels,mb_size,max_sequence_len)\n",
    "print(\"dev_accuracy: {}, dev avg_loss: {}\".format(0.8256880733944955,0.41458561278264455))\n",
    "print(\"test_accuracy: {}, test avg_loss: {}\".format(total_correct/total_pred,avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Analysis: Word Embeddings and the Attention Vector (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------largest 15 words with similarity-----\n",
      "never: 0.7919396758079529\n",
      "bad: 0.779381513595581\n",
      "not: 0.7775062322616577\n",
      "wrong: 0.7515763640403748\n",
      "less: 0.6921879649162292\n",
      "n't: 0.686932384967804\n",
      "drag: 0.6524785161018372\n",
      "missed: 0.6430465579032898\n",
      "inadvertent: 0.6347647309303284\n",
      "no: 0.6342142224311829\n",
      "nor: 0.6315809488296509\n",
      "falls: 0.6269453167915344\n",
      "too: 0.6262093186378479\n",
      "loud: 0.6106578707695007\n",
      "none: 0.6090220212936401\n",
      "\n",
      "------smallest 15 words with similarity------\n",
      ",: -0.9990133047103882\n",
      "that: -0.9986844062805176\n",
      "a: -0.9979445338249207\n",
      "it: -0.9977834224700928\n",
      "'s: -0.9958602786064148\n",
      "and: -0.9949108362197876\n",
      "to: -0.9870907664299011\n",
      "the: -0.9857791662216187\n",
      "is: -0.9839534163475037\n",
      "The: -0.9826574325561523\n",
      "--: -0.9804767966270447\n",
      "all: -0.9666380882263184\n",
      ".: -0.9649766087532043\n",
      "this: -0.9649695754051208\n",
      "in: -0.9637553095817566\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "for param in model2.parameters():\n",
    "    weights.append(param.data)\n",
    "embed_weights = weights[1]\n",
    "u = weights[0].view(1,100)\n",
    "cos_similarity =torch.nn.functional.cosine_similarity(embed_weights,u,dim=1).cpu().data.numpy()\n",
    "sorted_similarity = sorted(list(zip(range(len(cos_similarity)),cos_similarity)),key=lambda x: x[1])\n",
    "large =  sorted_similarity[-15:]\n",
    "small =  sorted_similarity[0:15] # account for 0\n",
    "\n",
    "print(\"------largest 15 words with similarity-----\")\n",
    "for i in range(15):\n",
    "    print(\"{}: {}\".format(tokenizer.index_word[large[14-i][0]],large[14-i][1]))\n",
    "print(\"\")\n",
    "print(\"------smallest 15 words with similarity------\")\n",
    "for i in range(15):\n",
    "    if small[i][0] != 0:\n",
    "        print(\"{}: {}\".format(tokenizer.index_word[small[i][0]],small[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Analysis: Variance of Attentions (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------largest 30 words with Variance -----\n",
      "create: 1.4647003412246704\n",
      "Like: 1.4148080348968506\n",
      "hero: 1.347995638847351\n",
      "seat: 1.3016000986099243\n",
      "New: 1.2686944007873535\n",
      "quality: 1.2547513246536255\n",
      "All: 1.2311038970947266\n",
      "kind: 1.2117339372634888\n",
      "interest: 1.192842960357666\n",
      "give: 1.1914682388305664\n",
      "talent: 1.1802623271942139\n",
      "war: 1.1794241666793823\n",
      "written: 1.178371548652649\n",
      "comes: 1.1640310287475586\n",
      "children: 1.1596442461013794\n",
      "sentimental: 1.1452240943908691\n",
      "full: 1.1182912588119507\n",
      "personal: 1.1128511428833008\n",
      "remains: 1.1107527017593384\n",
      "us: 1.1040270328521729\n",
      "man: 1.1016396284103394\n",
      "fans: 1.0930447578430176\n",
      "lives: 1.0926544666290283\n",
      "imagination: 1.0903058052062988\n",
      "everyone: 1.0865960121154785\n",
      "-RRB-: 1.0865715742111206\n",
      "theater: 1.0792664289474487\n",
      "visually: 1.0787477493286133\n",
      "moment: 1.0761202573776245\n",
      "classic: 1.072454571723938\n"
     ]
    }
   ],
   "source": [
    "cos = torch.nn.functional.cosine_similarity(embed_weights,u,dim=1)\n",
    "predictors2 = torch.stack(predictors).cuda()\n",
    "att_weights = torch.where(predictors2 != 0 ,cos[predictors2], torch.zeros_like(predictors2).float().cuda())\n",
    "att_weights = masked_softmax(att_weights,dim=1)\n",
    "pred = predictors2.cpu().numpy()\n",
    "att = att_weights.cpu().numpy()\n",
    "\n",
    "att_dict = {}\n",
    "for i in range(pred.shape[0]):\n",
    "    for j in range(pred.shape[1]):\n",
    "        if pred[i][j] in att_dict: \n",
    "            att_dict[pred[i][j]].append(att[i][j])\n",
    "        else: \n",
    "            att_dict[pred[i][j]] = [att[i][j]]  \n",
    "att_dict.pop(0,None)\n",
    "att_list = []\n",
    "for key, val in att_dict.items():\n",
    "    if len(val) >= 100:\n",
    "        val2 = np.array(val)\n",
    "        att_list.append((key,val2.std()/val2.mean()))\n",
    "    \n",
    "sorted_att = sorted(att_list,key=lambda x: x[1])\n",
    "large =  sorted_att[-30:]\n",
    "\n",
    "print(\"------largest 30 words with Variance -----\")\n",
    "for i in range(30):\n",
    "    print(\"{}: {}\".format(tokenizer.index_word[large[29-i][0]],large[29-i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple Self-Attention (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Implementation and Experimentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention without Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "total prediction: 67349, train_accuracy: 0.8930644849960653, train avg_loss: 0.28914963394176807\n",
      "total prediction: 872, validation_accuracy: 0.8038990825688074, validation avg_loss: 0.46741994397568143\n",
      "epoch: 1\n",
      "total prediction: 67349, train_accuracy: 0.9276752438788994, train avg_loss: 0.19371040116303223\n",
      "total prediction: 872, validation_accuracy: 0.8096330275229358, validation avg_loss: 0.5056266776019307\n",
      "epoch: 2\n",
      "total prediction: 67349, train_accuracy: 0.9414987601894609, train avg_loss: 0.1528761970414319\n",
      "total prediction: 872, validation_accuracy: 0.801605504587156, validation avg_loss: 0.5708180678734782\n",
      "epoch: 3\n",
      "total prediction: 67349, train_accuracy: 0.9514766366241518, train avg_loss: 0.12894692133284658\n",
      "total prediction: 872, validation_accuracy: 0.8084862385321101, validation avg_loss: 0.6132693307177669\n",
      "epoch: 4\n",
      "total prediction: 67349, train_accuracy: 0.956925863784169, train avg_loss: 0.11297883763044998\n",
      "total prediction: 872, validation_accuracy: 0.8107798165137615, validation avg_loss: 0.6930137940848276\n",
      "epoch: 5\n",
      "total prediction: 67349, train_accuracy: 0.961187248511485, train avg_loss: 0.1018632333096426\n",
      "total prediction: 872, validation_accuracy: 0.8038990825688074, validation avg_loss: 0.7452354806250097\n",
      "epoch: 6\n",
      "total prediction: 67349, train_accuracy: 0.9643350309581434, train avg_loss: 0.09289383720290754\n",
      "total prediction: 872, validation_accuracy: 0.7993119266055045, validation avg_loss: 0.8301823424958448\n",
      "epoch: 7\n",
      "total prediction: 67349, train_accuracy: 0.967601597648072, train avg_loss: 0.08528539805734905\n",
      "total prediction: 872, validation_accuracy: 0.7981651376146789, validation avg_loss: 0.8779847195880145\n",
      "epoch: 8\n",
      "total prediction: 67349, train_accuracy: 0.9696506258444817, train avg_loss: 0.07953494962036298\n",
      "total prediction: 872, validation_accuracy: 0.8027522935779816, validation avg_loss: 0.9430241499840568\n",
      "epoch: 9\n",
      "total prediction: 67349, train_accuracy: 0.9717145020713003, train avg_loss: 0.07361173454363314\n",
      "total prediction: 872, validation_accuracy: 0.7981651376146789, validation avg_loss: 1.0052571554991754\n",
      "181.24222469329834\n"
     ]
    }
   ],
   "source": [
    "# model implementation and training\n",
    "def masked_softmax(A, dim=1, epsilon=1e-5):\n",
    "    B = (A != 0).type(torch.DoubleTensor).cuda()\n",
    "    s = A.size()[0]\n",
    "    A_exp = torch.exp(A)\n",
    "    A_exp = A_exp * B # this step masks\n",
    "    A_softmax = A_exp / (torch.sum(A_exp,dim,keepdim=True)+epsilon)\n",
    "    \n",
    "    return A_softmax\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim,padding_idx=0).double()\n",
    "        self.embeddings.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.embeddings.weight.data[0] =  torch.zeros_like(self.embeddings.weight.data[0])\n",
    "        self.linear1 = nn.Linear(embedding_dim, 1,bias=False).double()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs,mb_size):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        A = torch.transpose(embeds,1,2)\n",
    "        A = torch.matmul(embeds, A)\n",
    "        att = torch.sum(A,dim=1)\n",
    "        att = masked_softmax(att,dim=1)\n",
    "\n",
    "        att =  torch.unsqueeze(att, 2)\n",
    "        att = att.repeat(1,1,100)\n",
    "        out = torch.mul(embeds, att)\n",
    "        out = torch.sum(out,dim=1)\n",
    "        out = self.linear1(out)\n",
    "        probs = self.sigmoid(out)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def evaluate2(model,predictors,labels,mb_size,max_sequence_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            total_pred = 0\n",
    "            total_correct = 0\n",
    "            total_loss =0 \n",
    "            sequences = [i for i in range(len(predictors))]\n",
    "            last_size = len(sequences) % mb_size\n",
    "            for mb in range(1+ len(sequences)//mb_size):\n",
    "                if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()    \n",
    "                else: \n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "                    mb_size = last_size \n",
    "                probs = model(batch.cuda(),max_sequence_len)\n",
    "                loss =  - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.0001).view(mb_size)))\n",
    "                total_loss += loss.item()\n",
    "                total_pred += mb_size \n",
    "                predict = list(map(lambda x : x >= 0.5,probs.view(mb_size).cpu().data.numpy()))\n",
    "                target = target.cpu().data.numpy() \n",
    "                for j in range(len(predict)):\n",
    "                    if predict[j] == target[j]:\n",
    "                        total_correct += 1     \n",
    "    model.train()    \n",
    "    return total_pred,total_correct,total_loss/total_pred\n",
    "    \n",
    "d =100\n",
    "\n",
    "model3 = Self_Attention(total_words, d)\n",
    "optimizer = optim.Adam(model3.parameters(), lr = 0.0003)\n",
    "model3 = model3.cuda()\n",
    " \n",
    "a = time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    mb_size = 32\n",
    "    sequences = [i for i in range(len(predictors))]\n",
    "    shuffle(sequences)\n",
    "    last_size = len(sequences) % mb_size\n",
    "    for mb in range(1+len(sequences)//mb_size):\n",
    "        if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()\n",
    "        else: \n",
    "            if last_size == 0:\n",
    "                break\n",
    "            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "            mb_size = last_size\n",
    "        model3.zero_grad()\n",
    "        probs = model3(batch.cuda(),max_sequence_len)\n",
    "        loss = - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.00001).view(mb_size)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.save(model3.state_dict(), \"./model_earlystop_\" + str(epoch))    \n",
    "    mb_size = 100\n",
    "    total_pred,total_correct,avg_loss= evaluate2(model3,predictors, labels,mb_size,max_sequence_len)\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    print(\"total prediction: {}, train_accuracy: {}, train avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "    total_pred,total_correct,avg_loss= evaluate2(model3,d_predictors, d_labels,mb_size,max_sequence_len)\n",
    "    print(\"total prediction: {}, validation_accuracy: {}, validation avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_accuracy: 0.8107798165137615, dev avg_loss: 0.6930137940848276\n",
      "test_accuracy: 0.8034047226798462, test avg_loss: 0.619780004929865\n"
     ]
    }
   ],
   "source": [
    "# test set evaluation\n",
    "model3 = Self_Attention(total_words, d)\n",
    "model3.cuda()\n",
    "model3.load_state_dict(torch.load(\"./model_earlystop_4\"))\n",
    "total_pred,total_correct,avg_loss= evaluate2(model3,t_predictors, t_labels,mb_size,max_sequence_len)\n",
    "print(\"dev_accuracy: {}, dev avg_loss: {}\".format(0.8107798165137615,0.6930137940848276))\n",
    "print(\"test_accuracy: {}, test avg_loss: {}\".format(total_correct/total_pred,avg_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "total prediction: 67349, train_accuracy: 0.8553207916969814, train avg_loss: 0.4039127049165434\n",
      "total prediction: 872, validation_accuracy: 0.783256880733945, validation avg_loss: 0.49383315046221965\n",
      "epoch: 1\n",
      "total prediction: 67349, train_accuracy: 0.9120996599801037, train avg_loss: 0.25111061599153783\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.4474375131036764\n",
      "epoch: 2\n",
      "total prediction: 67349, train_accuracy: 0.9320108687582592, train avg_loss: 0.19218755941552232\n",
      "total prediction: 872, validation_accuracy: 0.8084862385321101, validation avg_loss: 0.46405982689414643\n",
      "epoch: 3\n",
      "total prediction: 67349, train_accuracy: 0.9430874994431988, train avg_loss: 0.15905578937600126\n",
      "total prediction: 872, validation_accuracy: 0.8107798165137615, validation avg_loss: 0.48975637376359554\n",
      "epoch: 4\n",
      "total prediction: 67349, train_accuracy: 0.9500957697961365, train avg_loss: 0.1370567823563549\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.5219749716070143\n",
      "epoch: 5\n",
      "total prediction: 67349, train_accuracy: 0.9558716536251466, train avg_loss: 0.12118945607028543\n",
      "total prediction: 872, validation_accuracy: 0.8073394495412844, validation avg_loss: 0.5591811074032658\n",
      "epoch: 6\n",
      "total prediction: 67349, train_accuracy: 0.96016273441328, train avg_loss: 0.10941957133764257\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.5862457227669393\n",
      "epoch: 7\n",
      "total prediction: 67349, train_accuracy: 0.9636223254985227, train avg_loss: 0.10021736259032218\n",
      "total prediction: 872, validation_accuracy: 0.8084862385321101, validation avg_loss: 0.6272168304369459\n",
      "epoch: 8\n",
      "total prediction: 67349, train_accuracy: 0.9672007008270352, train avg_loss: 0.09274038806867506\n",
      "total prediction: 872, validation_accuracy: 0.805045871559633, validation avg_loss: 0.6641048999389043\n",
      "epoch: 9\n",
      "total prediction: 67349, train_accuracy: 0.9690567046281311, train avg_loss: 0.08640492824515753\n",
      "total prediction: 872, validation_accuracy: 0.805045871559633, validation avg_loss: 0.7109527557516458\n",
      "80.4847719669342\n"
     ]
    }
   ],
   "source": [
    "# model implementation and training\n",
    "def masked_softmax(A, dim=1, epsilon=1e-5):\n",
    "    B = (A != 0).type(torch.DoubleTensor).cuda()\n",
    "    A_exp = torch.exp(A)\n",
    "    A_exp = A_exp * B # this step masks\n",
    "    A_softmax = A_exp / (torch.sum(A_exp,dim,keepdim=True)+epsilon)\n",
    "    return A_softmax\n",
    "\n",
    "class Residual_Connection(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Residual_Connection, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim,padding_idx=0).double()\n",
    "        self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.embeddings.weight.data[0] =  torch.zeros_like(self.embeddings.weight.data[0])\n",
    "        self.linear1 = nn.Linear(embedding_dim, 1,bias=False).double()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs,mean_l):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        avg = torch.mean(embeds,dim=1)\n",
    "        avg = torch.mul(avg, mean_l)\n",
    "        A = torch.matmul(embeds, torch.transpose(embeds,1,2))\n",
    "        att = torch.sum(A,dim=1) \n",
    "        att = masked_softmax(att,dim=1)\n",
    "        att =  torch.unsqueeze(att, 2)\n",
    "        att = att.repeat(1,1,100)\n",
    "        out = torch.mul(embeds, att)\n",
    "        out = torch.sum(out,dim=1)\n",
    "        out = out + avg\n",
    "        out = self.linear1(out)\n",
    "        probs = self.sigmoid(out)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def evaluate3(model,predictors,labels,mb_size,mean_len,max_sequence_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            total_pred = 0\n",
    "            total_correct = 0\n",
    "            total_loss =0 \n",
    "            sequences = [i for i in range(len(predictors))]\n",
    "            last_size = len(sequences) % mb_size\n",
    "            for mb in range(1+len(sequences)//mb_size):\n",
    "                \n",
    "                if mb < len(sequences)//mb_size:\n",
    "                            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()\n",
    "                            mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).\\\n",
    "                                        view(mb_size,1).repeat(1,d).cuda()\n",
    "                else: \n",
    "                    if last_size == 0:\n",
    "                        break\n",
    "                    \n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.double).\\\n",
    "                                view(last_size,1).repeat(1,d).cuda()\n",
    "                    mb_size = last_size\n",
    "                probs = model(batch.cuda(),mean_l)\n",
    "                loss =  - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.0001).view(mb_size)))\n",
    "                total_loss += loss.item()\n",
    "                total_pred += mb_size \n",
    "                predict = list(map(lambda x : x >= 0.5,probs.view(mb_size).cpu().data.numpy() ))\n",
    "                target = target.cpu().data.numpy() \n",
    "                for j in range(len(predict)):\n",
    "                    if predict[j] == target[j]:\n",
    "                        total_correct += 1     \n",
    "    model.train()    \n",
    "    return total_pred,total_correct,total_loss/total_pred\n",
    "    \n",
    "d =100\n",
    "\n",
    "model4 = Residual_Connection(total_words, d)\n",
    "optimizer = optim.Adam(model4.parameters(), lr = 0.0003)\n",
    "model4 = model4.cuda()\n",
    " \n",
    "a = time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    mb_size = 100\n",
    "    sequences = [i for i in range(len(predictors))]\n",
    "    shuffle(sequences)\n",
    "    last_size = len(sequences) % mb_size\n",
    "    for mb in range(1+len(sequences)//mb_size):\n",
    "        if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).\\\n",
    "                                view(mb_size,1).repeat(1,d).cuda()\n",
    "        else: \n",
    "            if last_size == 0:\n",
    "                break\n",
    "            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "            mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.double).\\\n",
    "                        view(last_size,1).repeat(1,d).cuda()\n",
    "            mb_size = last_size\n",
    "        model4.zero_grad()\n",
    "        probs = model4(batch.cuda(),mean_l)\n",
    "        loss = - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.00001).view(mb_size)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    mb_size = 100\n",
    "    torch.save(model4.state_dict(), \"./model_earlystop_\" + str(epoch))    \n",
    "    total_pred,total_correct,avg_loss= evaluate3(model4,predictors, labels,mb_size,mean_len,max_sequence_len)\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    print(\"total prediction: {}, train_accuracy: {}, train avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "    total_pred,total_correct,avg_loss= evaluate3(model4,d_predictors, d_labels,mb_size,d_mean_len,max_sequence_len)\n",
    "    print(\"total prediction: {}, validation_accuracy: {}, validation avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_accuracy: 0.8119266055045872, dev avg_loss: 0.5219749716070143\n",
      "test_accuracy: 0.8110928061504667, test avg_loss: 0.432737020717729\n"
     ]
    }
   ],
   "source": [
    "# test set evaluation\n",
    "model4 = Residual_Connection(total_words, d)\n",
    "model4.cuda()\n",
    "model4.load_state_dict(torch.load(\"./model_earlystop_3\"))\n",
    "total_pred,total_correct,avg_loss= evaluate3(model4,t_predictors, t_labels,mb_size,t_mean_len,max_sequence_len)\n",
    "print(\"dev_accuracy: {}, dev avg_loss: {}\".format(0.8119266055045872,0.5219749716070143))\n",
    "print(\"test_accuracy: {}, test avg_loss: {}\".format(total_correct/total_pred,avg_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Enriching the Attention Function (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "total prediction: 67349, train_accuracy: 0.8789588561077373, train avg_loss: 0.33624531412840963\n",
      "total prediction: 872, validation_accuracy: 0.7901376146788991, validation avg_loss: 0.46895605635812876\n",
      "epoch: 1\n",
      "total prediction: 67349, train_accuracy: 0.9246759417363287, train avg_loss: 0.2092989992390989\n",
      "total prediction: 872, validation_accuracy: 0.8027522935779816, validation avg_loss: 0.47445440058949256\n",
      "epoch: 2\n",
      "total prediction: 67349, train_accuracy: 0.943265675808104, train avg_loss: 0.1573031131694194\n",
      "total prediction: 872, validation_accuracy: 0.8038990825688074, validation avg_loss: 0.5132261945575513\n",
      "epoch: 3\n",
      "total prediction: 67349, train_accuracy: 0.953555360881379, train avg_loss: 0.12787920629052996\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.546792625309283\n",
      "epoch: 4\n",
      "total prediction: 67349, train_accuracy: 0.9601775824436889, train avg_loss: 0.10836914784203588\n",
      "total prediction: 872, validation_accuracy: 0.8165137614678899, validation avg_loss: 0.590056316658234\n",
      "epoch: 5\n",
      "total prediction: 67349, train_accuracy: 0.9651962167218519, train avg_loss: 0.09439554552105633\n",
      "total prediction: 872, validation_accuracy: 0.8176605504587156, validation avg_loss: 0.6364916900036274\n",
      "epoch: 6\n",
      "total prediction: 67349, train_accuracy: 0.9695615376620291, train avg_loss: 0.08363146319493499\n",
      "total prediction: 872, validation_accuracy: 0.8176605504587156, validation avg_loss: 0.6911812784956569\n",
      "epoch: 7\n",
      "total prediction: 67349, train_accuracy: 0.97267962404787, train avg_loss: 0.07543310531236908\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.7353019791258432\n",
      "epoch: 8\n",
      "total prediction: 67349, train_accuracy: 0.9750107648220464, train avg_loss: 0.06832382301314152\n",
      "total prediction: 872, validation_accuracy: 0.8119266055045872, validation avg_loss: 0.8008880613796064\n",
      "epoch: 9\n",
      "total prediction: 67349, train_accuracy: 0.9768370725623247, train avg_loss: 0.0622557134139598\n",
      "total prediction: 872, validation_accuracy: 0.8096330275229358, validation avg_loss: 0.8607573829818937\n",
      "212.3748438358307\n"
     ]
    }
   ],
   "source": [
    "#I embedded position and sentence length, and concatenate them with the word embedding \n",
    "\n",
    "def masked_softmax(A, dim=1, epsilon=1e-5):\n",
    "    B = (A != 0).type(torch.DoubleTensor).cuda()\n",
    "    A_exp = torch.exp(A)\n",
    "    A_exp = A_exp * B # this step masks\n",
    "    A_softmax = A_exp / (torch.sum(A_exp,dim,keepdim=True)+epsilon)\n",
    "    return A_softmax\n",
    "\n",
    "class Enriched_Att(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Enriched_Att, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim,padding_idx=0).double()\n",
    "        self.embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.embeddings.weight.data[0] =  torch.zeros_like(self.embeddings.weight.data[0])\n",
    "        \n",
    "        self.embeddings2 = nn.Embedding(56, 40,padding_idx=0).double() # position\n",
    "        self.embeddings3 = nn.Embedding(56, 10,padding_idx=0).double()# sent length\n",
    "        \n",
    "        self.embeddings2.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.embeddings3.weight.data.uniform_(-0.05, 0.05)\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim//2 + embedding_dim, 1,bias=False).double()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, inputs,mean_l):\n",
    "        \n",
    "        pos = torch.tensor([[i for i in range(56)] for j in range(inputs.shape[0])],dtype=torch.long).cuda()\n",
    "        pos_embeds =  self.embeddings2(pos)\n",
    "        \n",
    "        embeds = self.embeddings(inputs)\n",
    "        B = (inputs != 0).type(torch.DoubleTensor).cuda()\n",
    "        C = B.sum(dim=1).long()\n",
    "        \n",
    "        B = B.unsqueeze(2).repeat(1,1,50)\n",
    "        C = C.unsqueeze(1).repeat(1,56)\n",
    "        \n",
    "        len_embeds = self.embeddings3(C)\n",
    "        \n",
    "        pos_embeds = torch.cat((pos_embeds,len_embeds),2)\n",
    "        pos_embeds = B * pos_embeds\n",
    "        embeds = torch.cat((embeds,pos_embeds),2)\n",
    "        avg = torch.mean(embeds,dim=1)\n",
    "        avg = torch.mul(avg, mean_l)\n",
    "        A = torch.matmul(embeds, torch.transpose(embeds,1,2))\n",
    "        self_att = torch.sum(A,dim=1)         \n",
    "        att = self_att  \n",
    "        att = masked_softmax(att,dim=1)\n",
    "        att =  torch.unsqueeze(att, 2)\n",
    "        att = att.repeat(1,1,150)\n",
    "        \n",
    "        out = torch.mul(embeds, att)\n",
    "        out = torch.sum(out,dim=1)\n",
    "        out = out + avg\n",
    "        out = self.linear1(out)\n",
    "        \n",
    "        probs = self.sigmoid(out)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def evaluate3(model,predictors,labels,mb_size,mean_len,max_sequence_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            total_pred = 0\n",
    "            total_correct = 0\n",
    "            total_loss =0 \n",
    "            sequences = [i for i in range(len(predictors))]\n",
    "            last_size = len(sequences) % mb_size\n",
    "            for mb in range(1+len(sequences)//mb_size):\n",
    "                \n",
    "                if mb < len(sequences)//mb_size:\n",
    "                            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()\n",
    "                            mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).\\\n",
    "                                        view(mb_size,1).repeat(1,150).cuda()\n",
    "                else: \n",
    "                    if last_size == 0:\n",
    "                        break\n",
    "                    \n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.double).\\\n",
    "                                view(last_size,1).repeat(1,150).cuda()\n",
    "                    mb_size = last_size\n",
    "                probs = model(batch.cuda(),mean_l)\n",
    "                loss =  - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.0001).view(mb_size)))\n",
    "                total_loss += loss.item()\n",
    "                total_pred += mb_size \n",
    "                predict = list(map(lambda x : x >= 0.5,probs.view(mb_size).cpu().data.numpy() ))\n",
    "                target = target.cpu().data.numpy() \n",
    "                for j in range(len(predict)):\n",
    "                    if predict[j] == target[j]:\n",
    "                        total_correct += 1     \n",
    "    model.train()    \n",
    "    return total_pred,total_correct,total_loss/total_pred\n",
    "    \n",
    "d =100\n",
    "\n",
    "model4 = Enriched_Att(total_words, d)\n",
    "optimizer = optim.Adam(model4.parameters(), lr = 0.0003)\n",
    "model4 = model4.cuda()\n",
    "\n",
    "a = time()\n",
    "\n",
    "for epoch in range(10):\n",
    "    mb_size = 50\n",
    "    sequences = [i for i in range(len(predictors))]\n",
    "    shuffle(sequences)\n",
    "    last_size = len(sequences) % mb_size\n",
    "    for mb in range(1+len(sequences)//mb_size):\n",
    "        #print(mb)\n",
    "        if mb < len(sequences)//mb_size:\n",
    "                    batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:(mb+1)*mb_size])).cuda()\n",
    "                    target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).cuda()\n",
    "                    mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:(mb+1)*mb_size]),dtype=torch.double).\\\n",
    "                                view(mb_size,1).repeat(1,150).cuda()\n",
    "        else: \n",
    "            if last_size == 0:\n",
    "                break\n",
    "            batch = torch.stack(list(predictors[i] for i in sequences[mb*mb_size:])).cuda()\n",
    "            target = torch.tensor(list(labels[i] for i in sequences[mb*mb_size:]),dtype=torch.double).cuda()\n",
    "            mean_l = torch.tensor(list(mean_len[i] for i in sequences[mb*mb_size:]),dtype=torch.double).\\\n",
    "                        view(last_size,1).repeat(1,150).cuda()\n",
    "            mb_size = last_size\n",
    "        model4.zero_grad()\n",
    "        probs = model4(batch.cuda(),mean_l)\n",
    "        loss = - torch.sum(target*torch.log(probs).view(mb_size)) - torch.sum(((1-target)*torch.log(1-probs+0.00001).view(mb_size)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    mb_size = 100\n",
    "    torch.save(model4.state_dict(), \"./model_earlystop_\" + str(epoch))    \n",
    "    total_pred,total_correct,avg_loss= evaluate3(model4,predictors, labels,mb_size,mean_len,max_sequence_len)\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    print(\"total prediction: {}, train_accuracy: {}, train avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "    total_pred,total_correct,avg_loss= evaluate3(model4,d_predictors, d_labels,mb_size,d_mean_len,max_sequence_len)\n",
    "    print(\"total prediction: {}, validation_accuracy: {}, validation avg_loss: {}\".format(total_pred,total_correct/total_pred,avg_loss))\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_accuracy: 0.8119266055045872, dev avg_loss: 0.5219749716070143\n",
      "test_accuracy: 0.8215266337177375, test avg_loss: 0.4196136991457575\n"
     ]
    }
   ],
   "source": [
    "# test set evaluation\n",
    "model4 = Enriched_Att(total_words, d)\n",
    "model4.cuda()\n",
    "model4.load_state_dict(torch.load(\"./model_earlystop_5\"))\n",
    "total_pred,total_correct,avg_loss= evaluate3(model4,t_predictors, t_labels,mb_size,t_mean_len,max_sequence_len)\n",
    "print(\"dev_accuracy: {}, dev avg_loss: {}\".format(0.8176605504587156,0.6364916900036274))\n",
    "print(\"test_accuracy: {}, test avg_loss: {}\".format(total_correct/total_pred,avg_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
